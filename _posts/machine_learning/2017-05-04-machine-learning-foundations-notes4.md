---
layout:     post
title:      "【机器学习】机器学习基石学习笔记4：机器学习的可行性"
date:       2017-05-04 12:03:00
categories: MachineLearning
tags:  machine-learning notes Machine-Learning-Foundations
author: Tandy
---

* content
{:toc}

这篇博客主要是学习国立台湾大学林轩田的机器学习课程过程中的一些笔记






## 课程大纲

- 该课程从机器学习做不到的一些事入手，通过蓝橙小球的例子说明可以通过已知来推算未知，并证明这个说法的可行性，提出其正确的前提条件是假说数量有限，抽样的资料足够多

## 学习解决不了的问题

- 有争议的问题

![][1]

- 在训练样本集（in-sample）中，可以求得一个最佳的假设g，该假设最大可能的接近目标函数f，但是在训练样本集之外的其他样本（out-of-sample）中，假设g和目标函数f可能差别很远。

![][2]

## 解决一个概率问题

## 弹珠例子

- 有一个罐子，这个罐子里盛放着橙色和绿色两种颜色的小球，我们如何在不查遍所有小球的情况下，得知罐子中橙子小球所占的比例呢？抽取样本，如下图所示。根据统计的知识，我们只要通过抽样，根据样本中的比例就可以推算罐子中的比例。

![][3]

## PAC

- 根据Hoeffding公式，可以证明v=u是可能近似正确（PAC）的，尤其是当抽样数目N很大时，这样我们就有很大概率地从已知的v推测出未知的u

![][4]

## 回到学习

### 弹珠实验和机器学习

- 其实前文中提到的例子可以和机器学习问题一一对应：

罐子小球 | 机器学习 
--------|---------
未知的橙色小球比例 | 某一确定的假设在整个X输入空间中，输入向量x满足条件 的占整个输入空间的比例  
抽取的小球∈整个罐子中的小球 | 训练输入样本集 整个数据集X 
橙色小球 | 假设h作用于此输入向量x与给定的输出不相等
绿色小球 | 假设h作用于此输入向量x与给定的输出相等
小球样本是从罐子中独立随机抽取的 | 输入样本x是从整个数据集D中独立随机选择的

![][5]

### Ein和Eout

- 未知的Eout：h和f在整个罐子中一不一样（u）
- 已知的Ein：h和f在资料总一不一样（v）
- P：罐子，产生资料的地方

![][6]

### 公式证明

- 通过公式证明：h在我们手上的样本的错误率和在整个罐子里的错误率是接近的，是PAC（可能近似正确）的

![][7]

- 对于一个假说h，可以证明
	- 如果Ein很小：g和f很接近
	- 如果Ein很大：g和f不接近

![][8]

### 小结

- 到这里，我们可以通过历史数据来验证一个备选的h是不是好的，这里只能验证，还不能做选择

![][9]

## 真正的学习

### 如何选择

- 真正的学习不止有验证，还有选择，如何选择一个好的h，直接学Ein最小的吗？作者通过下面的例子说明不能这能简单来选

![][10]


- 丢铜板的例子同理：有选择时就会有偏见，当选择多的时候，小概率事件发生的概率也会变大

![][11]

### 不好的资料

- 上面的例子很形象，每一个罐子都是一个假设集合，我们默认是挑表现最好的，也就是全绿色（错误率为0）的那个假设。但是当从众多假设选择时，得到全对的概率也在增加，就像丢硬币一样，当有个150个童鞋同时丢硬币5次，那么这些人中出现5面同时朝上的概率为99%，所以表现好的有可能是小概率事件发生（毕竟对于每个假设其泛化能力是PAC），其不一定就有好的泛化能力（Ein和Eout相同），我们称这次数据是坏数据（可以理解为选到了泛化能力差的假设），在坏数据上，Ein和Eout的表现是差别很大的，这就是那个小概率事件，Hoeffding's inequality告诉我们，每个h在采样数据上Ein和Eout差别很大的概率很低（坏数据）
- 坏数据=没有自由的选择=对于一些存在的h，Ein和Eout差得很多

![][12]

- 接下来求不好的资料的概率是多少？（有一个h不好就是不好）
- 最终的结论：我们每次选取Ein最小的h就是合理的，因为如果M小N大，出现表现好的坏数据的假设几率降低了，我们选择表现后就有信心认为其有良好的泛化能力。

![][13]

### 小结

- 前提：h只有有限多种选择，资料量N够多
- 则：不管演算法怎么选，Ein和Eout都会接近，即机器学习是可以做到的
- 所以：要选一个Ein最小的
- 那无限条的假说的情况的，以后再说

![][14]

## 总结

- 整体证明机器可以学习分了两个层面，首先对于单个假设，根据Hoeffding不等式，当N很大时，其泛化能力强是PAC的；而实际上机器学习是从众多假设中挑Ein最小的（通过测试集找）假设，这个的理论基础是当M不大(有限)，N大，选到泛化能力差的假设概率低（用到了单个假设的结论）。

- 这节课的理论证明比较多，目的是证明如何学习（在有限的假说集合中找尽可能多的数据来验证），最终的结论还是比较直观的。

![][15]


[1]: {{ site.baseurl }}/images/201706/50.png
[2]: {{ site.baseurl }}/images/201706/51.png
[3]: {{ site.baseurl }}/images/201706/52.png
[4]: {{ site.baseurl }}/images/201706/53.png
[5]: {{ site.baseurl }}/images/201706/54.png
[6]: {{ site.baseurl }}/images/201706/55.png
[7]: {{ site.baseurl }}/images/201706/56.png
[8]: {{ site.baseurl }}/images/201706/57.png
[9]: {{ site.baseurl }}/images/201706/58.png
[10]: {{ site.baseurl }}/images/201706/59.png
[11]: {{ site.baseurl }}/images/201706/60.png
[12]: {{ site.baseurl }}/images/201706/61.png
[13]: {{ site.baseurl }}/images/201706/62.png
[14]: {{ site.baseurl }}/images/201706/63.png
[15]: {{ site.baseurl }}/images/201706/64.png

## 参考资料

- [Machine Learning Foundations](http://www.csie.ntu.edu.tw/~htlin/mooc/)

- [机器学习的可能性](http://www.cnblogs.com/HappyAngel/p/3495804.html)

- [在何时可以使用机器学习(4)](http://www.cnblogs.com/ymingjingr/p/4276386.html)