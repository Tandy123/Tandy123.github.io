---
layout:     post
title:      "【机器学习】机器学习基石学习笔记4：机器学习的可行性"
date:       2017-05-20 12:02:00
categories: MachineLearning
tags:  machine-learning notes Machine-Learning-Foundations
author: Tandy
---

* content
{:toc}

这篇博客主要是学习国立台湾大学林轩田的机器学习课程过程中的一些笔记






## 课程大纲

- 该课程从机器学习做不到的一些事入手，通过蓝橙小球的例子说明可以通过已知来推算未知，并证明这个说法的可行性，提出其正确的前提条件是假说数量有限，抽样的资料足够多

## 学习解决不了的问题

- 有争议的问题

![][1]

- 在训练样本集（in-sample）中，可以求得一个最佳的假设g，该假设最大可能的接近目标函数f，但是在训练样本集之外的其他样本（out-of-sample）中，假设g和目标函数f可能差别很远。

![][2]

## 解决一个概率问题

## 弹珠例子

- 有一个罐子，这个罐子里盛放着橙色和绿色两种颜色的小球，我们如何在不查遍所有小球的情况下，得知罐子中橙子小球所占的比例呢？抽取样本，如下图所示。根据统计的知识，我们只要通过抽样，根据样本中的比例就可以推算罐子中的比例。

![][3]

## PAC

- 根据Hoeffding公式，可以证明v=u是可能近似正确（PAC）的，尤其是当抽样数目N很大时，这样我们就有很大概率地从已知的v推测出未知的u

![][4]

## 回到学习

### 弹珠实验和机器学习

| 罐子小球 | 机器学习 
|未知的橙色小球比例|某一确定的假设在整个X输入空间中，输入向量x满足条件 的占整个输入空间的比例

抽取的小球∈整个罐子中的小球

训练输入样本集 整个数据集X

橙色小球

假设h作用于此输入向量x与给定的输出不相等

绿色小球

假设h作用于此输入向量x与给定的输出相等

小球样本是从罐子中独立随机抽取的

输入样本x是从整个数据集D中独立随机选择的


![][5]

### Ein和Eout

- 未知的Eout：h和f在整个罐子中一不一样（u）
- 已知的Ein：h和f在资料总一不一样（v）
- P：罐子，产生资料的地方

![][6]

### 公式证明

- 通过公式证明：h在我们手上的样本的错误率和在整个罐子里的错误率是接近的，是PAC（可能近似正确）的

![][7]

- 对于一个假说h，可以证明
	- 如果Ein很小：g和f很接近
	- 如果Ein很大：g和f不接近

![][8]

### 小结

- 到这里，我们可以通过历史数据来验证一个备选的h是不是好的，这里只能验证，还不能做选择

![][9]

## 真正的学习

### 如何选择

- 真正的学习不止有验证，还有选择，如何选择一个好的h，直接学Ein最小的吗？作者通过下面的例子说明不能这能简单来选

![][10]


- 丢铜板的例子同理：有选择时就会有偏见，当选择多的时候，小概率事件发生的概率也会变大

![][11]

### 不好的资料

- 不好的资料=没有自由的选择=对于一些存在的h，Ein和Eout差得很多

![][12]

- 接下来求不好的资料的概率是多少？（有一个h不好就是不好）
- 最终的结论：我们应该选一个g，这个g的Ein最小，则Eout也很可能比较小

![][13]

### 小结

- 前提：h只有有限多种选择，资料量N够多
- 则：不管演算法怎么选，Ein和Eout都会接近
- 所以：要选一个Ein最小的，即在这种情况下（h只有有限多种选择，资料量N够多），机器学习是可以做到的
- 那无限条的假说的情况的，以后再说

![][14]

### 小结

![][15]

## 根据不同的输入空间X分类
 
### 具体特征

- 带有人类智慧预处理之后的描述，处理起来比较简单

![][16]

### 抽象特征

- 数据的原始特征，比较抽象，处理起来比较困难

- 以字体识别为例，具体特征就是字写得是否对称，字的密度
- 抽象特征是16*16的图像构成的256维的空间

![][17]

### 抽取特征：

![][18]

### 小结

- 通常简单地假设我们已经有了具体特征
- 大部分情况下，数据中三种特征（具体，抽象，原始）都有

![][19]

## 总结

![][20]

[1]: {{ site.baseurl }}/images/201706/50.png
[2]: http://farm5.staticflickr.com/4232/34439442214_e180647825_b.jpg
[3]: {{ site.baseurl }}/images/201706/32.png
[4]: {{ site.baseurl }}/images/201706/33.png
[5]: {{ site.baseurl }}/images/201706/34.png
[6]: {{ site.baseurl }}/images/201706/35.png
[7]: {{ site.baseurl }}/images/201706/36.png
[8]: {{ site.baseurl }}/images/201706/37.png
[9]: {{ site.baseurl }}/images/201706/38.png
[10]: {{ site.baseurl }}/images/201706/39.png
[11]: {{ site.baseurl }}/images/201706/40.png
[12]: {{ site.baseurl }}/images/201706/41.png
[13]: {{ site.baseurl }}/images/201706/42.png
[14]: {{ site.baseurl }}/images/201706/43.png
[15]: {{ site.baseurl }}/images/201706/44.png
[16]: {{ site.baseurl }}/images/201706/45.png
[17]: {{ site.baseurl }}/images/201706/46.png
[18]: {{ site.baseurl }}/images/201706/47.png
[19]: {{ site.baseurl }}/images/201706/48.png
[20]: {{ site.baseurl }}/images/201706/49.png

## 参考资料

- [Machine Learning Foundations](http://www.csie.ntu.edu.tw/~htlin/mooc/)




